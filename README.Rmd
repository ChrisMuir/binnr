
```{r set-options, echo=FALSE, cache=FALSE}
options(width=120)
library(binnr)
```

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.path = "plots/README-",
  cache=TRUE,
  width=12
)
```

# What is `binnr`?
`binnr` is a package that creates, manages, and applies simple binning
transformations. It makes scorecard modeling easy and fast. Using `binnr`,
a modeler can discretize continuous variables, expand & collapse bins,
and apply monotonicity constraints all from within an easy-to-use
interface.

`binnr` not only discretizes variables and provides functions for 
manipulating them, but also performs weight-of-evidence substitution
on a dataset, transforming all predictors to the same scale and 
making them continuous. This has a number of benefits:

1. Subsituting weight-of-evidence forces a linear relationship between
the predictor and the binary target - Ideal for logistic regression
2. Continuous features can be used in more training algorithms
3. Missing values are also substituted creating a data set of complete
cases

When paired with penalized regression techniques such as ridge or LASSO
regression, a model can be quickly created and fine-tuned to completion 
in a fraction of the time of traditional modeling techniques. All
of this with no loss (and often a gain) in predictive performance.

### Binning Algorithm

The binning algorithm used by `binnr` is completely writtin in `C` and 
is very fast. It uses a supervised disretization method based on 
information value to make recursive splits of the data. The algorithm
support monotonicity constraints, exception values, and missing values.

#### Monotonicity

`binnr` supports 4 types of monotonicity within the `C` implementation.
Each type of constratint is specified by a special integer value.

| **Value** | **Meaning** |
|---------|-----------|
| 0 | No montonicity |
| -1 | Decreasing y as x increases |
| 1 | Increasing y as x increases |
| 2 | Either increasing or decreasing y as x increases |

Of special note is the value of 2. The algorithm implements this by 
making the first split in *any* direction and then uses that 
direction for the rest of the splits. This often results in the best
monotonic relationship without specifying the direction apriori.

#### Exception Values

`binnr` also supports exception values for each variable. Exceptions
only apply to continuous variables. The algorithm does not collapse exception
values but *does* use them to calculate information value statistics.

#### Missing Values

Missing values are handled by excluding them entirely from the binning
step. They do not inform the binning process at all. Missing values
are substituted with zeros when performing weight-of-evidence
substitution.

## Modeling with `binnr` Overview

The basic workflow of building a scorecard with `binnr` is comprised of
a few basic steps:

1. Bin the dataset using the `bin` function
  * Use `summary` to see a high-level view of the binned data
  * Look for variables that should not be modeled
2. Perform the weight-of-evidence substition on the data
3. Fit a LASSO regression model
4. Use `adjust` on the final model variables to tweak them
5. Repeat steps 3-4 until satisfied

Each of these steps will be detailed further below with examples.

### Bin the data

A small dataset containig a variety of variable types is included with 
the `binnr` package. It consists of 891 passengers on the Titanic, their 
survival status, and several demographic and socioeconomic attributes.
This dataset will be used throught this help document.

```{r, echo = TRUE}
data(titanic)
head(titanic)
```

Binning the data is as simple as calling the `bin` function on a `data.frame`.
The `bin` function accepts several arguments that control the binning
algorithm:

| **Argument** | **Controls** | **Example**|
|--------------|--------------|------------|
| min.iv | Minimum IV increase to split data | `min.iv = .01` |
| min.cnt | Mininmum # Obs in bins after splitting | `min.cnt = 100` |
| max.bin | Maximum # Bins excluding exceptions and missing | `max.bin = 10` |
| mono | Monotonicity relationship between x and y | `mono = c(Fare=1, Pclass=2)` |
| exceptions | List of exception values for each x | `exceptions = list(ALL=-1)` |

The `mono` argument accepts a named *vector* of values. The special name `ALL` 
applies to all of the variables. Monotonicity is applied on where names match.
Similarly, `exceptions` accepts a named *list* of values. Because variables can
have multiple exception values, each entry can be a vector. Like, `mono`, the
reserved name, `ALL`, applies the exceptions to each variable.

#### Examples using `mono` and `exceptions`

| **Example** | **Explanation** |
|---|---|
| `mono = c(ALL=1, Fare=2)` | Bin Fare in any monotonic direction; bin the rest with positive montonicity
| `exceptions = list(ALL = -1, Age = c(-99, -100))` | Exclude -99 and -100 when binning Age, exclude -1 for the rest of the variables |
| `mono = c(ALL=2)` | Bin all variables monotonically in any direction |

```{r, echo = TRUE, eval=TRUE, results='hide'}
bins <- bin(titanic[,-1], titanic$Survived)
```

This returns a `bin.list` object which contains a `bin` object for every
column of the dataset. Each `bin` object contains all of the information 
necessary to perform manipulations, plot data, and perform weight of 
evidence substitutions. Printing a `bin.list` object prints a summary of 
what it contains.

```{r, echo = TRUE, eval=TRUE}
bins
```

There are 7 bins contained within the `bin.list` object - 3 discreted and 4
continuous. The distinction between discrete and continuous bins will be
demonstrated when using the `adjust` function.

Because `bin.list` is a list underneath, individual bins can be accessed 
in the normal list indexing manner. Printing a single bin produces a WoE
table with detailed statistics about the binned and target variables

```{r, echo = TRUE, eval=TRUE}
bins$Pclass
```

Furthermore, a `bin.list` may also be subset just like a base R list:

```{r, echo = TRUE, eval=TRUE}
bins[1:4]
```

Calling the summary function on a `bin.list` returns a `data.frame`
of high level information about each binned attribute:

```{r, echo = TRUE, eval=TRUE}
s <- summary(bins)
print(s)
```

The summary is sorted by descending information value placing the 
most predictive attribuets at the top of the list. The summary
`data.frame` can be used to identify variables that should not be
modeled. For example, a discrete variable with 40 bins should be 
collapsed before using.

### Apply Weight-of-Evidence Substitutions

`binnr` provides a `predict` function that is used to perform the WoE 
substitution on a `data.frame`. The columns are matched by name and a
matrix of numeric values is returned.

```{r, echo = TRUE, results='hide'}
binned <- predict(bins, titanic)
```

```{r, echo = TRUE}
head(binned)
```

Creating a table of the WoE-substituted values with the original values
illustrates what `binnr` is doing behind the scenes:

```{r, echo = TRUE}
table(titanic$Pclass, round(binned[,'Pclass'], 3))
```
We can verify that values of `male` are being coded correctly to the value found
in the WoE table. The same holds true for `female`.

### Logistic Regression

Once the variable transoformations have been applied, a logistic regression
model may be fit. We will be applying a new logistic regression algorithm called
`LASSO`. It fits the model and performs variable selection at the same time.
More about LASSO regression can be found [here](http://statweb.stanford.edu/~tibs/lasso.html).

LASSO regression requires that we specify a penalty argument to constrain the 
coefficients. We will be using cross-validation to determine this parameter
automatically. Furthermore, since our variables are already transformed the way
we like, we will also force the parameters to be greater than zero. This will
prevent any "flips" from occuring in our final model.

And here is the raw variable crossed with the transformed variable:
```{r, echo = FALSE}
library(glmnet)
```

```{r, echo = TRUE, cache=TRUE}
fit <- cv.glmnet(binned, titanic$Survived, alpha=1, family="binomial",
                 nfolds = 5, lower.limits=0)
plot(fit)
```

The resulting plot shows the error on the y-axis and the penalty term on the
x-axis. The penalty term controls the size of the coefficients and how many of
them are not equal to zero. The first dashed line represents the size of the
penalty term that has the lowest cross-validation error. We can access this
value easily by using the "lambda.min" argument where appropriate. For example, 
to find the optimal coefficients:

```{r, echo = TRUE}
coef(fit, s="lambda.min")
```



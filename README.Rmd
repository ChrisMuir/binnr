
```{r set-options, echo=FALSE, cache=FALSE}
options(width=120)
library(binnr)
```

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "",
  fig.path = "plots/README-",
  cache=TRUE,
  width=12
)
```

## What is `binner`?
`binnr` is a package that creates, manages, and applies simple binning
transformations. It makes scorecard modeling easy and fast. Using `binnr`,
a modeler can discretize continuous variables, expand & collapse bins,
and apply monotonicity constraints all from within an easy-to-use
interface.

`binnr` not only discretizes variables and provides functions for 
manipulating them, but also performs weight-of-evidence substitution
on an a dataset transforming all predictors to the same scale and 
making them continuous. This has a number of benefits:

1. Continuous features can be used in more training algorithms
2. The WoE substitution replaces the input variable with its associated
log-odds -- ideal for modeling with logistic regression
3. Missing values are also substituted creating a data set of complete
cases

When paired with penalized regression techniques such as ridge or LASSO
regression, a model can be quickly created and fine-tuned to completion 
in a fraction of the time of traditional modeling techniques. All
of this with no loss (and often a gain) in predictive performance.

## Binning Algorithm

The binning algorithm used by `binnr` is completely writtin in `C` and 
is very fast. It uses a supervised disretization method based on 
information value to make recursive splits of the data. The algorithm
support monotonicity constraints, exception values, and missing values.

### Monotonicity

`binnr` supports 4 types of monotonicity within the `C` implementation.
Each type of constratint is specified by a special integer value.

| *Value* | *Meaning* |
|---------|-----------|
| 0 | No montonicity |
| -1 | Decreasing y as x increases |
| 1 | Increasing y as x increases |
| 2 | Either increasing or decreasing y as x increases |

Of special note is the value of 2. The algorithm implements this by 
making the first split in **any** direction and then uses that 
direction for the rest of the splits. This often results in the best
monotonic relationship without specifying the direction apriori.

## Overview

The basic workflow of building a scorecard with `binnr` is comprised of
a few basic steps:

1. Bin the dataset using the `bin` function
  * Use `summary` to see a high-level view of the binned data
  * Look for variables that should not be modeled
2. Perform the weight-of-evidence substition on the data
3. Fit a LASSO regression model
4. Use `adjust` on the final model variables to tweak them
5. Repeat steps 3-4 until satisfied

Each of these steps will be detailed further below with examples.

## Bin the data

A small dataset containig a variety of variable types is included with 
the `binnr` package. It consists of 891 passengers on the Titanic, their 
survival status, and several demographic and socioeconomic attributes.
This dataset will be used throught this help document.

```{r, echo = TRUE}
data(titanic)
head(titanic)
```

Binning the data is as simple as calling the `bin` function on a `data.frame`.

```{r, echo = TRUE, eval=TRUE, results='hide'}
bins <- bin(titanic[,-1], titanic$Survived)
```

This returns a `bin.list` object which contains a `bin` object for every
column of the dataset. Each `bin` object contains all of the information 
necessary to perform manipulations, plot data, and perform weight of 
evidence substitutions. Printing a `bin.list` object prints a summary of 
what it contains.

```{r, echo = TRUE, eval=TRUE}
bins
```

There are 7 bins contained within the `bin.list` object - 3 discreted and 4
continuous. The distinction between discrete and continuous bins will be
demonstrated when using the `adjust` function.

Because `bin.list` is a list underneath, individual bins can be accessed 
in the normal list indexing manner. Printing a single bin produces a WoE
table with detailed statistics about the binned and target variables

```{r, echo = TRUE, eval=TRUE}
bins$Pclass
```

Furthermore, a `bin.list` may also be subset just like a base R list:

```{r, echo = TRUE, eval=TRUE}
bins[1:4]
```

## Apply Weight-of-Evidence Substitutions

`binnr` provides a `predict` function that is used to perform the WoE 
substitution on a `data.frame`. The columns are matched by name and a
matrix of numeric values is returned.

```{r, echo = TRUE, results='hide'}
binned <- predict(bins, titanic)
```

```{r, echo = TRUE}
head(binned)
```


Creating a table of the WoE-substituted values with the original values
illustrates what `binnr` is doing behind the scenes:

```{r, echo = TRUE}
table(titanic$Pclass, round(binned[,'Pclass'], 3))
```
We can verify that values of `male` are being coded correctly to the value found
in the WoE table. The same holds true for `female`.

### Logistic Regression

Once the variable transoformations have been applied, a logistic regression
model may be fit. We will be applying a new logistic regression algorithm called
`LASSO`. It fits the model and performs variable selection at the same time.
More about LASSO regression can be found [here](http://statweb.stanford.edu/~tibs/lasso.html).

LASSO regression requires that we specify a penalty argument to constrain the 
coefficients. We will be using cross-validation to determine this parameter
automatically. Furthermore, since our variables are already transformed the way
we like, we will also force the parameters to be greater than zero. This will
prevent any "flips" from occuring in our final model.

And here is the raw variable crossed with the transformed variable:
```{r, echo = FALSE}
library(glmnet)
```

```{r, echo = TRUE, cache=TRUE}
fit <- cv.glmnet(binned, titanic$Survived, alpha=1, family="binomial",
                 nfolds = 5, lower.limits=0)
plot(fit)
```

The resulting plot shows the error on the y-axis and the penalty term on the
x-axis. The penalty term controls the size of the coefficients and how many of
them are not equal to zero. The first dashed line represents the size of the
penalty term that has the lowest cross-validation error. We can access this
value easily by using the "lambda.min" argument where appropriate. For example, 
to find the optimal coefficients:

```{r, echo = TRUE}
coef(fit, s="lambda.min")
```


